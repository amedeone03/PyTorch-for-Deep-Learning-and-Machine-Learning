ubuntu@mtk-genio:~/Desktop/test/benchmark_dla$ python3 benchmark2.py --count 100
2024-10-03 14:19:01,192 [INFO] Available backends: ['mdla2.0', 'vpu_fpu']
2024-10-03 14:19:01,199 [INFO] Running inference on mdla2.0
Traceback (most recent call last):
  File "/home/ubuntu/Desktop/test/benchmark_dla/benchmark2.py", line 184, in <module>
    benchmark_tflite_model(args.file, backend, args.count, args.options, profile=args.profile)
  File "/home/ubuntu/Desktop/test/benchmark_dla/benchmark2.py", line 96, in benchmark_tflite_model
    tflite_2_dla(model, target, dla, compile_options)
  File "/home/ubuntu/Desktop/test/benchmark_dla/benchmark2.py", line 90, in tflite_2_dla
    res = subprocess.run(batcmd, shell=True, check=True, executable='/bin/bash', stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)
  File "/usr/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'ncc-tflite -arch mdla2.0 yolov5x_quantized.tflite -o yolov5x_quantized-mdla2.0.dla ' returned non-zero exit status 1.
------------------------------
import tensorflow as tf
import numpy as np

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="yolov5x_quantized.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Generate random input data for testing
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)

# Run inference
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()

# Get the output
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)

------------------------------------

import tensorflow as tf
import numpy as np
import time

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path="yolov5x_quantized.tflite")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Generate random input data for testing
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)

# Run inference multiple times to calculate average inference time
num_inferences = 100  # Number of times to run inference
total_time = 0.0

for _ in range(num_inferences):
    interpreter.set_tensor(input_details[0]['index'], input_data)
    
    # Measure time for each inference
    start_time = time.time()
    interpreter.invoke()
    end_time = time.time()
    
    total_time += (end_time - start_time)

average_inference_time = total_time / num_inferences
print(f"Average inference time over {num_inferences} runs: {average_inference_time:.6f} seconds")

# Get the output from the model
output_data = interpreter.get_tensor(output_details[0]['index'])
print("Output data:", output_data)
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
COSE DA FARE:
- aggiornare risultati ottenuti col codice (per tensorflow lite su CPU) (da una parte onnx dall altra tflite)
- scrivere readme di risultati con benchmark.py che si ottengono 
- scrivere il procedimento per runnare inferenza con mdla2.0 e vpu_fpu
