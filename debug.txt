
ubuntu@mtk-genio:~/Desktop/test/benchmark_dla$ python3 -m tf2onnx.convert --opset 13 --input yolov5x.onnx --output yolov5x_tf_saved_model --output-format saved_model
/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
usage: convert.py [-h] [--input INPUT] [--graphdef GRAPHDEF] [--saved-model SAVED_MODEL] [--tag TAG] [--signature_def SIGNATURE_DEF] [--concrete_function CONCRETE_FUNCTION]
                  [--checkpoint CHECKPOINT] [--keras KERAS] [--tflite TFLITE] [--tfjs TFJS] [--large_model] [--output OUTPUT] [--inputs INPUTS] [--outputs OUTPUTS]
                  [--ignore_default IGNORE_DEFAULT] [--use_default USE_DEFAULT] [--rename-inputs RENAME_INPUTS] [--rename-outputs RENAME_OUTPUTS] [--use-graph-names]
                  [--opset OPSET] [--dequantize] [--custom-ops CUSTOM_OPS] [--extra_opset EXTRA_OPSET] [--load_op_libraries LOAD_OP_LIBRARIES]
                  [--target {rs4,rs5,rs6,caffe2,tensorrt,nhwc}] [--continue_on_error] [--verbose] [--debug] [--output_frozen_graph OUTPUT_FROZEN_GRAPH]
                  [--inputs-as-nchw INPUTS_AS_NCHW] [--outputs-as-nchw OUTPUTS_AS_NCHW]
convert.py: error: unrecognized arguments: --output-format saved_model



ubuntu@mtk-genio:~/Desktop/test/benchmark_dla$ python3 quantize.py 
Traceback (most recent call last):
  File "/home/ubuntu/Desktop/test/benchmark_dla/quantize.py", line 6, in <module>
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py", line 2073, in from_saved_model
    saved_model = _load(saved_model_dir, tags)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py", line 900, in load
    result = load_partial(export_dir, None, tags, options)["root"]
  File "/home/ubuntu/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py", line 1005, in load_partial
    loader_impl.parse_saved_model_with_debug_info(export_dir))
  File "/home/ubuntu/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/loader_impl.py", line 59, in parse_saved_model_with_debug_info
    saved_model = parse_saved_model(export_dir)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/tensorflow/python/saved_model/loader_impl.py", line 119, in parse_saved_model
    raise IOError(
OSError: SavedModel file does not exist at: yolov5x.tflite/{saved_model.pbtxt|saved_model.pb}

-----------------------------------------------------------------------------

import onnx
from onnxruntime.quantization import QuantType, QuantizationMode,quantize_static, QuantFormat,CalibrationDataReader
import onnxruntime
import cv2 
import os 
import numpy as np

def format_yolov5(frame):
        row, col, _ = frame.shape
        _max = max(col, row)
        result = np.zeros((_max, _max, 3), np.uint8)
        result[0:row, 0:col] = frame
        return result

def _preprocess_images(images_folder: str, height: int, width: int, size_limit=0):
    """
    Loads a batch of images and preprocess them
    parameter images_folder: path to folder storing images
    parameter height: image height in pixels
    parameter width: image width in pixels
    parameter size_limit: number of images to load. Default is 0 which means all images are picked.
    return: list of matrices characterizing multiple images
    """
    image_names = os.listdir(images_folder)
    if size_limit > 0 and len(image_names) >= size_limit:
        batch_filenames = [image_names[i] for i in range(size_limit)]
    else:
        batch_filenames = image_names
    unconcatenated_batch_data = []

    for image_name in batch_filenames:
        image_filepath = images_folder + "/" + image_name
        pillow_img = cv2.imread(image_filepath)
        pillow_img = format_yolov5(pillow_img)
        nchw_data = cv2.dnn.blobFromImage(pillow_img, 1 / 255.0, (height,width), swapRB=True)
        unconcatenated_batch_data.append(nchw_data)
        
    batch_data = numpy.concatenate(
        numpy.expand_dims(unconcatenated_batch_data, axis=0), axis=0)
    
           
    return batch_data


class yolov5_cal_data_reader(CalibrationDataReader):
    def __init__(self, calibration_image_folder: str, model_path: str):
        self.enum_data = None

        # Use inference session to get input shape.
        session = onnxruntime.InferenceSession(model_path, None)
        (_, _, height, width) = session.get_inputs()[0].shape

        # Convert image to input data
        self.nhwc_data_list = _preprocess_images(
            calibration_image_folder, height, width, size_limit=0
        )
        self.input_name = session.get_inputs()[0].name
        self.datasize = len(self.nhwc_data_list)

    def get_next(self):
        if self.enum_data is None:
            self.enum_data = iter(
                [{self.input_name: nhwc_data} for nhwc_data in self.nhwc_data_list]
            )
        return next(self.enum_data, None)

    def rewind(self):
        self.enum_data = None

model_path = './best.onnx'
outout_model_quant = './bests.quant.onnx'

-----------------------------------------------------------------------------------------

import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

# Path to your original ONNX model
model_fp32 = './best.onnx'

# Path to save the quantized ONNX model
model_quant = './best_quantized.onnx'

# Perform dynamic quantization (weights are quantized, activations remain float)
quantize_dynamic(model_fp32, model_quant, weight_type=QuantType.QInt8)

print(f"Quantized ONNX model saved as: {model_quant}")

----------------------------------------------------------------------------------------

import onnxruntime as ort
import numpy as np

# Load the quantized ONNX model
quantized_model_path = "best_quantized.onnx"  # Replace with your quantized model path
session = ort.InferenceSession(quantized_model_path)

# Get input and output details
input_name = session.get_inputs()[0].name
input_shape = session.get_inputs()[0].shape
output_name = session.get_outputs()[0].name

# Generate a random input tensor with the correct shape
input_data = np.random.rand(*input_shape).astype(np.float32)

# Run inference
outputs = session.run([output_name], {input_name: input_data})

# Print the output
print(f"Inference result from quantized model: {outputs}")

